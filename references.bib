@misc{MetasploitUnleashed,
title = {{Metasploit Unleashed}},
url = {https://www.offensive-security.com/metasploit-unleashed/}
}
@misc{Metasploit,
title = {{Metasploit}},
url = {https://www.metasploit.com/}
}
@inproceedings{Kancherla,
abstract = {Malware detection is one of the challenging tasks in Cyber security. The advent of code obfuscation, metamorphic malware, packers and zero day attacks has made malware detection a challenging task. In this paper we present a visualization based approach for malware detection. First the executable is converted to a gray-scale image called byteplot. Later we extract low level features like intensity based and texture based features. We apply computationally intelligent techniques for malware detection using these features. In this work we used Support Vector Machines (SVMs) and obtained an accuracy of 95{\%} on a dataset containing 25000 malware and 12000 benign samples.},
author = {Kancherla, Kesav and Mukkamala, Srinivas},
booktitle = {2013 IEEE Symposium on Computational Intelligence in Cyber Security (CICS)},
title = {{Image visualization based malware detection}},
url = {https://ieeexplore.ieee.org/document/6597204},
year = {2013}
}
@misc{make,
author = {Stallman, Richard and McGrath, Roland and Smith, Paul},
title = {{GNU make}},
url = {https://www.gnu.org/software/make/manual/make.html}
}
@article{Eykholt2017,
abstract = {Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations.Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm,Robust Physical Perturbations (RP2), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. Witha perturbation in the form of only black and white stickers,we attack a real stop sign, causing targeted misclassification in 100{\%} of the images obtained in lab settings, and in 84.8{\%}of the captured video frames obtained on a moving vehicle(field test) for the target classifier.},
archivePrefix = {arXiv},
arxivId = {1707.08945},
author = {Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
eprint = {1707.08945},
file = {:home/wayne/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Eykholt et al. - 2017 - Robust Physical-World Attacks on Deep Learning Models.pdf:pdf},
journal = {arXiv},
month = {jul},
title = {{Robust Physical-World Attacks on Deep Learning Models}},
url = {http://arxiv.org/abs/1707.08945},
year = {2017}
}
@article{Simonyan2013,
abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
archivePrefix = {arXiv},
arxivId = {1312.6034},
author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
eprint = {1312.6034},
file = {:home/wayne/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Vedaldi, Zisserman - 2013 - Deep Inside Convolutional Networks Visualising Image Classification Models and Saliency Maps.pdf:pdf},
journal = {arXiv},
month = {dec},
pages = {1--8},
title = {{Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}},
url = {http://arxiv.org/abs/1312.6034},
year = {2013}
}
@inproceedings{Caruana2015,
abstract = {In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30-day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.},
address = {New York, New York, USA},
author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '15},
doi = {10.1145/2783258.2788613},
file = {:home/wayne/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Caruana et al. - 2015 - Intelligible Models for HealthCare.pdf:pdf},
isbn = {9781450336642},
pages = {1721--1730},
publisher = {ACM Press},
title = {{Intelligible Models for HealthCare}},
url = {http://dl.acm.org/citation.cfm?doid=2783258.2788613},
year = {2015}
}
@article{Sgandurra,
abstract = {The Internet of Things (IoT) is an extension of the traditional Internet, which allows a very large number of smart devices, such as home appliances, network cameras, sensors and controllers to connect to one another to share information and improve user experiences. Current IoT devices are typically micro-computers for domain-specific computations rather than traditional functionspecific embedded devices. Therefore, many existing attacks, targeted at traditional computers connected to the Internet, may also be directed at IoT devices. For example, DDoS attacks have become very common in IoT environments, as these environments currently lack basic security monitoring and protection mechanisms, as shown by the recent Mirai and Brickerbot IoT botnets. In this paper, we propose a novel light-weight approach for detecting DDos malware in IoT environments.We firstly extract one-channel gray-scale images converted from binaries, and then utilize a lightweight convolutional neural network for classifying IoT malware families. The experimental results show that the proposed system can achieve 94.0{\%} accuracy for the classification of goodware and DDoS malware, and 81.8{\%} accuracy for the classification of goodware and two main malware families.},
archivePrefix = {arXiv},
arxivId = {1802.03714},
author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Prasad, Sanjiva and Sgandurra, Daniele and Feng, Yaokai and Sakurai, Kouichi},
eprint = {1802.03714},
file = {:home/wayne/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sgandurra - Unknown - Lightweight Classification of IoT Malware based on Image Recognition.pdf:pdf},
journal = {arXiv},
keywords = {convolutional neural network,internet of things,light-weight detection,malware image,or,or hard copies of,part or all of,permission to make digital,this work for personal},
month = {feb},
title = {{Lightweight Classification of IoT Malware based on Image Recognition}},
url = {http://arxiv.org/abs/1802.03714},
year = {2018}
}
@article{Raff2017,
abstract = {In this work we introduce malware detection from raw byte sequences as a fruitful research area to the larger machine learning community. Building a neural network for such a problem presents a number of interesting challenges that have not occurred in tasks such as image processing or NLP. In particular, we note that detection from raw bytes presents a sequence problem with over two million time steps and a problem where batch normalization appear to hinder the learning process. We present our initial work in building a solution to tackle this problem, which has linear complexity dependence on the sequence length, and allows for interpretable sub-regions of the binary to be identified. In doing so we will discuss the many challenges in building a neural network to process data at this scale, and the methods we used to work around them.},
archivePrefix = {arXiv},
arxivId = {1710.09435},
author = {Raff, Edward and Barker, Jon and Sylvester, Jared and Brandon, Robert and Catanzaro, Bryan and Nicholas, Charles},
eprint = {1710.09435},
file = {:home/wayne/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raff et al. - 2017 - Malware Detection by Eating a Whole EXE.pdf:pdf},
journal = {arXiv},
month = {oct},
title = {{Malware Detection by Eating a Whole EXE}},
url = {http://arxiv.org/abs/1710.09435},
year = {2017}
}
@article{SantacroceSalience,
author = {Santacroce, Michael and Stegner, Wayne and Koranek, Daniel and Jha, Rashmi},
file = {:home/wayne/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Santacroce et al. - Unknown - A Foray Into Extracting Malicious Features from Executable Code with Neural Network Salience.pdf:pdf},
journal = {NAECON},
title = {{A Foray Into Extracting Malicious Features from Executable Code with Neural Network Salience}},
year = {2018}
}
@article{SantacroceMaV,
abstract = {We explore a novel method for transforming exe-cutable bytecode into a video rather than an image for classification with deep, time-distributed neural networks, achieving up to 99.86{\%} training and 98.74{\%} testing accuracy on 9 classes of malware, and up to 99.99{\%} training and 99.36{\%} testing accuracy on malicious vs. benign files. The network could also classify all malware in our dataset for a false positive rate of 13{\%}. Our input is only the source code of the files without any other information. We then explore methods for pruning and quantizing the network so that it may be more feasible for widespread implementation. We introduce a novel pruning method, Node-Distance Pruning, and then attempt a generalized rule for the number of quantization levels per layer.},
author = {Santacroce, M and Koranek, D and Jha, R},
file = {:home/wayne/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Santacroce, Koranek, Jha - Unknown - Exploring the Detection of Malware Code as Video with Compressed, Time-Distributed CNNs.pdf:pdf},
journal = {Submitted but Unpublished},
keywords = {deep learning,machine learning,malware detection,network compression,pruning,video classi-fication},
title = {{Exploring the Detection of Malware Code as Video with Compressed, Time-Distributed CNNs}},
year = {2018}
}
@article{Selvaraju2017,
abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multimodal inputs (e.g. VQA) or reinforcement learning, without any architectural changes or re-training. We combine GradCAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes (showing that seemingly unreasonable predictions have reasonable explanations), (b) are robust to adversarial images, (c) outperform previous methods on weakly-supervised localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, our visualizations show that even non-attention based models can localize inputs. Finally, we conduct human studies to measure if GradCAM explanations help users establish trust in predictions from deep networks and show that GradCAM helps untrained users successfully discern a "stronger" deep network from a "weaker" one. Our code is available at https://github.com/ramprs/grad-cam. A demo and a video of the demo can be found at http://gradcam.cloudcv.org and youtu.be/COjUB9Izk6E.},
archivePrefix = {arXiv},
arxivId = {arXiv:1610.02391v3},
author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
doi = {10.1109/ICCV.2017.74},
eprint = {arXiv:1610.02391v3},
file = {:home/wayne/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2017 - Grad-CAM Visual Explanations from Deep Networks via Gradient-Based Localization.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {618--626},
title = {{Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization}},
volume = {2017-Octob},
year = {2017}
}
@article{Biggers2014,
abstract = {Feature location is a program comprehension activity, the goal of which is to identify source code entities that implement a functionality. Recent feature location techniques apply text retrieval models such as latent Dirichlet allocation (LDA) to corpora built from text embedded in source code. These techniques are highly configurable, and the literature offers little insight into how different configurations affect their performance. In this paper we present a study of an LDA based feature location technique (FLT) in which we measure the performance effects of using different configurations to index corpora and to retrieve 618 features from 6 open source Java systems. In particular, we measure the effects of the query, the text extractor configuration, and the LDA parameter values on the accuracy of the LDA based FLT. Our key findings are that exclusion of comments and literals from the corpus lowers accuracy and that heuristics for selecting LDA parameter values in the natural language context are suboptimal in the source code context. Based on the results of our case study, we offer specific recommendations for configuring the LDA based FLT.},
archivePrefix = {arXiv},
arxivId = {1708.08296v1},
author = {Biggers, Lauren R. and Bocovich, Cecylia and Capshaw, Riley and Eddy, Brian P. and Etzkorn, Letha H. and Kraft, Nicholas A.},
doi = {10.1007/s10664-012-9224-x},
eprint = {1708.08296v1},
file = {:home/wayne/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Samek, Wiegand, M{\"{u}}ller - 2017 - Explainable Artificial Intelligence Understanding, Visualizing and Interpreting Deep Learning Models.pdf:pdf},
isbn = {1066401292},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Feature location,Program comprehension,Software evolution,Static analysis,Text retrieval},
number = {3},
pages = {465--500},
pmid = {1458562},
title = {{Explainable AI: understanding, visualizing and interpreting deep learning models}},
volume = {19},
year = {2014}
}
@misc{ELF,
booktitle = {Specification, Unix System Laboratories},
file = {:home/wayne/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/TEMP{\_}AUTHOR - 2001 - Executable and Linkable Format (ELF).pdf:pdf},
title = {{Executable and Linkable Format (ELF)}},
url = {http://www.skyfree.org/linux/references/ELF{\_}Format.pdf},
year = {2001}
}
@article{Ronen2018,
abstract = {The Microsoft Malware Classification Challenge was announced in 2015 along with a publication of a huge dataset of nearly 0.5 terabytes, consisting of disassembly and bytecode of more than 20K malware samples. Apart from serving in the Kaggle competition, the dataset has become a standard benchmark for research on modeling malware behaviour. To date, the dataset has been cited in more than 50 research papers. Here we provide a high-level comparison of the publications citing the dataset. The comparison simplifies finding potential research directions in this field and future performance evaluation of the dataset.},
archivePrefix = {arXiv},
arxivId = {1802.10135},
author = {Ronen, Royi and Radu, Marian and Feuerstein, Corina and Yom-Tov, Elad and Ahmadi, Mansour},
eprint = {1802.10135},
file = {:home/wayne/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronen et al. - 2018 - Microsoft Malware Classification Challenge.pdf:pdf},
journal = {arXiv},
month = {feb},
title = {{Microsoft Malware Classification Challenge}},
url = {http://arxiv.org/abs/1802.10135},
year = {2018}
}
@article{Gunning2015,
abstract = {Based on research into the applications of artificial intelligence (AI) technology in the manufacturing industry in recent years, we analyze the rapid development of core technologies in the new era of 'Internet plus AI', which is triggering a great change in the models, means, and ecosystems of the manufacturing industry, as well as in the development of AI. We then propose new models, means, and forms of intelligent manufacturing, intelligent manufacturing system architecture, and intelligent man-ufacturing technology system, based on the integration of AI technology with information communications, manufacturing, and related product technology. Moreover, from the perspectives of intelligent manufacturing application technology, industry, and application demonstration, the current development in intelligent manufacturing is discussed. Finally, suggestions for the appli-cation of AI in intelligent manufacturing in China are presented.},
author = {Vagg, M and Leach, MJ},
doi = {10.1111/fct.12208},
file = {:home/wayne/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leach - 2015 - Authors' reply.pdf:pdf},
isbn = {6000000200},
issn = {14653753},
journal = {Focus on Alternative and Complementary Therapies},
month = {dec},
number = {3-4},
pages = {172--173},
pmid = {30082306},
title = {{Potentially useful study of transcendental meditation fails to impress because of poor methodology}},
url = {http://doi.wiley.com/10.1111/fct.12208},
volume = {20},
year = {2015}
}
